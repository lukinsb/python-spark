{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <font size=\"6\">Python-Spark</font></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Qual o objetivo do comando <b> cache </b> em Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O comando cache instrui ao Spark da necessidade de carregar um arquivo, stream de dados ou outro tipo de informação, em memória. Após solicitar um arquivo do armazenamento ou qualquer outro tipo de processo por meio do spark, para que estes dados sejam armazenados em memória para que sejam utilizados posteriormente é necessário que se utilize o comando cache por meio da sintaxe: *.cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2- O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em MapReduce. Por quê?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O principal motivo pelo qual um código implementado em Spark é, normalmente, 100x mais rápido que a sua implementação em Hadoop com MapReduce quando em memória, ou 10x mais rápido em disco, se deve ao fato de todo o processamento em Spark ser realizado diretamente na memória principal do node Spark em que o código está rodando, não necessitando de todo o stack de trocas de informação (I/O operations, operações de troca de dados) entre o Hadoop e o hardware. Portanto, a utilização do Spark reduz muito operações em disco, que se tornam desnecessárias pelo método com que este roda os códigos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Qual a função do <b>Spark Context</b>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Spark context é \"portão de entrada\" de todas as funcionalidades do Apache Spark. Gerar o Spark Context é uma das principais tarefas esperadas de uma aplicação Spark. O Spark Context possibilita o acesso ao Cluster Spark e é ccriado após o comando SparkConf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Explique com suas palavras o que é <b>Resilient Distributed Datasets</b> (RDD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD's são a estrutura fundamental de dados do Spark, a forma como o Spark representa e armazena seus dados de forma distribuida em diversas máquinas (ou nodes), englobando, também, suas API's tornando possível acessar esses dados. Devido a descentralização dos dados é possível garantir alta taxa de persistência e disponibilidade dos dados, visto que são armazenados de forma redundante e como um \"dataset\" particionado por diversos servidores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - <b>GroupByKey</b> é menos eficiente que <b>reduceByKey</b> em grandes datasets. Por quê?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em maiores datasets o comando \"reduceByKey\" tem melhor desempenho, visto que com ele o Spark consegue combinar os resultados diferentes partições em uma mesma \"chave comum\", equanto o \"groupByKey\" somente agrupa o dataset resultando em uma combinação dos dados antes da sua separação entre os clusteres pelo RDD. O \"reduceByKey\" pode-ser dizer que agrupa e agrega os dados, por um indice, sem movê-los realmente, mantendo-os distribuidos o comando realiza menor movimentação dos dados economizando memória e operações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 - O que o código Scala abaixo faz?\n",
    "\n",
    "![Código em Scala](image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código descrito acima realiza as seguintes funções em descrição linha a linha:\n",
    "\n",
    "1. Cria a variável textFile e armazena o conteudo de um arquivo texto carregado de um sistema de dados distribuidos HDFS para a mesma;\n",
    "\n",
    "2. Cria a variável counts e faz seu \"assign\" para itens \"mapeados\" (extraidos a partir de regras) do texto carregado na variável textFile, os valores mapeados para counts serão as palavras contidas no texto, porém separando-as palavra a palavra em cada linha por meio de seus espaços.\n",
    "\n",
    "3. Dentro do map anterior cada linha recebe outro comando map, com cada palavra sendo colocada em um arranjo (tupla) numérico, com um índice para cada palavra\n",
    "\n",
    "4. Ainda dentro do primeiro map é feita uma redução (reduce) com os itens sendo agragados por underscores.\n",
    "\n",
    "5. A variável counts é armazenada como um arquivo de texto após a realização dos comandos Map e Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> HTTP requests to NASA Kennedy Space Center WWW server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "e_Pattern = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+)\\s*(\\S*)\\s*\" (\\d{3}) (\\S+)'\n",
    "\n",
    "\n",
    "dataset1 = io.open(\"access_log_Jul95\", \"r+\", encoding=\"latin1\").read()\n",
    "dataset2 = io.open(\"access_log_Aug95\", \"r+\", encoding=\"latin1\").read()\n",
    "dataset = dataset1+dataset2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.split('\\n')\n",
    "dataset = {'raw': dataset}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['host'] = df['raw'].str.extract('^(\\S+)', expand=True)\n",
    "df['timestamp'] = df['raw'].str.extract('\\[([\\w:/]+\\s[+\\-]\\d{4})\\]', expand=True)\n",
    "df['request_Type'] = df['raw'].str.extract('\"(\\S+) (\\S+)\\s*(\\S*)\\s*\"', expand=True)[0]\n",
    "df['request_Path'] = df['raw'].str.extract('\"(\\S+) (\\S+)\\s*(\\S*)\\s*\"', expand=True)[1]\n",
    "df['request_Status'] = df['raw'].str.extract('\"(\\S+) (\\S+)\\s*(\\S*)\\s*\"', expand=True)[2]\n",
    "df['reply'] = df['raw'].str.extract('\" (\\d{3})', expand=True)\n",
    "df['bytes'] = df['raw'].str.extract('\" (\\d{3}) (\\S+)', expand=True)[1]\n",
    "df['date'] = df['timestamp'].str.slice(0,11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('raw', axis=1)\n",
    "df = df.drop(3461612)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hosts = len(df['host'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_errors = df[df['reply'].str.contains(\"404\")].count().reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_errors = df[df['reply'].str.contains(\"404\")].request_Path.value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_day = df[df['reply'].str.contains(\"404\")].date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bytes = pd.to_numeric(df['bytes'], errors = 'coerce')\n",
    "total_bytes = df['bytes'].sum(skipna = True)/1073741824"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Numero de hosts unicos: 137979 \n",
      "\n",
      "2. Numero total de erros 404: 20901 \n",
      "\n",
      "3. As 5 URLs que mais causaram erros 404: \n",
      "/pub/winvn/readme.txt                           2004\n",
      "/pub/winvn/release.txt                          1732\n",
      "/shuttle/missions/STS-69/mission-STS-69.html     683\n",
      "/shuttle/missions/sts-68/ksc-upclose.gif         428\n",
      "/history/apollo/a-001/a-001-patch-small.gif      384\n",
      "Name: request_Path, dtype: int64 \n",
      "\n",
      "4. Quantidade de erros 404 por dia: \n",
      "06/Jul/1995    640\n",
      "19/Jul/1995    639\n",
      "30/Aug/1995    571\n",
      "07/Jul/1995    570\n",
      "07/Aug/1995    537\n",
      "13/Jul/1995    532\n",
      "31/Aug/1995    526\n",
      "05/Jul/1995    497\n",
      "03/Jul/1995    474\n",
      "12/Jul/1995    471\n",
      "11/Jul/1995    471\n",
      "18/Jul/1995    465\n",
      "25/Jul/1995    461\n",
      "20/Jul/1995    428\n",
      "24/Aug/1995    420\n",
      "29/Aug/1995    420\n",
      "25/Aug/1995    415\n",
      "14/Jul/1995    413\n",
      "28/Aug/1995    410\n",
      "17/Jul/1995    406\n",
      "10/Jul/1995    398\n",
      "08/Aug/1995    391\n",
      "06/Aug/1995    373\n",
      "27/Aug/1995    370\n",
      "26/Aug/1995    366\n",
      "04/Jul/1995    359\n",
      "09/Jul/1995    348\n",
      "04/Aug/1995    346\n",
      "23/Aug/1995    345\n",
      "27/Jul/1995    336\n",
      "26/Jul/1995    336\n",
      "21/Jul/1995    334\n",
      "24/Jul/1995    328\n",
      "15/Aug/1995    327\n",
      "01/Jul/1995    316\n",
      "10/Aug/1995    315\n",
      "20/Aug/1995    312\n",
      "21/Aug/1995    305\n",
      "03/Aug/1995    304\n",
      "08/Jul/1995    302\n",
      "02/Jul/1995    291\n",
      "22/Aug/1995    288\n",
      "14/Aug/1995    287\n",
      "09/Aug/1995    279\n",
      "17/Aug/1995    271\n",
      "11/Aug/1995    263\n",
      "16/Aug/1995    259\n",
      "16/Jul/1995    257\n",
      "18/Aug/1995    256\n",
      "15/Jul/1995    254\n",
      "01/Aug/1995    243\n",
      "05/Aug/1995    236\n",
      "23/Jul/1995    233\n",
      "13/Aug/1995    216\n",
      "19/Aug/1995    209\n",
      "12/Aug/1995    196\n",
      "22/Jul/1995    192\n",
      "28/Jul/1995     94\n",
      "Name: date, dtype: int64 \n",
      "\n",
      "5. Total de bytes retornados: 61.0243 GB\n"
     ]
    }
   ],
   "source": [
    "print('1. Numero de hosts unicos: %d \\n' % unique_hosts)\n",
    "print('2. Numero total de erros 404: %d \\n' % total_errors)\n",
    "print('3. As 5 URLs que mais causaram erros 404: \\n%s \\n' % url_errors)\n",
    "print('4. Quantidade de erros 404 por dia: \\n%s \\n' % errors_day)\n",
    "print('5. Total de bytes retornados: %.4f %s' % (total_bytes, 'GB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfGPU",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
